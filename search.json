[
  {
    "objectID": "posts/intro_to_mass_spec/4_reanalysis/index.html",
    "href": "posts/intro_to_mass_spec/4_reanalysis/index.html",
    "title": "Part 4: Introduction to Analysis and Plotting Mass Spectrometry Data in R",
    "section": "",
    "text": "This is a continuation from Part 3."
  },
  {
    "objectID": "posts/intro_to_mass_spec/4_reanalysis/index.html#plot-a-profile-spectrum",
    "href": "posts/intro_to_mass_spec/4_reanalysis/index.html#plot-a-profile-spectrum",
    "title": "Part 4: Introduction to Analysis and Plotting Mass Spectrometry Data in R",
    "section": "Plot a profile spectrum",
    "text": "Plot a profile spectrum\nThe two-column matrix can be passed directly to R‚Äôs base plotting function, with the additional argument type=\"l\" which stands for ‚Äúline‚Äù.\n\nplot(\n  single_spectrum,\n  type = \"l\",\n)"
  },
  {
    "objectID": "posts/intro_to_mass_spec/4_reanalysis/index.html#plot-an-isotopic-envelope",
    "href": "posts/intro_to_mass_spec/4_reanalysis/index.html#plot-an-isotopic-envelope",
    "title": "Part 4: Introduction to Analysis and Plotting Mass Spectrometry Data in R",
    "section": "Plot an isotopic envelope",
    "text": "Plot an isotopic envelope\nLet‚Äôs zoom in on the 13C isotopic envelope of the ‚Äú743.5622 m/z‚Äù precursor we are interested in. Note that R/code doesn‚Äôt have a concept of ‚Äúzoom‚Äù, we just filter the data for the the area of interest and only plot that filtered data. Here we filter the data to only include rows where the mass column is greater than 743 and less than 750.\n\nggplot(\n  data = subset(single_spectrum, mass &gt; 743 & mass &lt; 750),\n  aes(\n    x = mass, \n    y = intensity\n  )\n) + \n  geom_line(color=\"gray48\") + \n  geom_point(size = 0.75, color=\"gray0\")\n\n\n\n\n\n\n\n\nAnd now zoom in further, to the monoisotopic peak.\n\nggplot(\n  data = subset(single_spectrum, mass &gt; 743 & mass &lt; 744),\n  aes(\n    x = mass, \n    y = intensity\n  )\n) + \n  geom_line(color=\"gray48\") + \n  geom_point(size = 0.75, color=\"gray0\") \n\n\n\n\n\n\n\n\nAn important thing to take note of when doing most types of spectroscopy/spectrometry is the number of measurements across a peak. Here we are getting ~10 data points per ion/peak which is pretty good. The smaller the number of points, the worse your peak shape will be, the worse your accuracy and precision will be. Alternately, too many points can bloat your data size and sometimes make analyses more difficult. Additionally, more data points collected per scan can decrease the number of scans you can collect per unit of time. This is largely controlled by dwell time in the MS acquisition settings.\n\nggplot(\n  data = subset(single_spectrum, mass &gt; 743 & mass &lt; 744),\n  aes(\n    x = mass, \n    y = intensity\n  )\n) + \n  geom_line(color=\"gray48\") + \n  geom_point(size = 0.75, color=\"gray0\") +\n  geom_point(data = subset(subset(single_spectrum, mass &gt; 743 & mass &lt; 744), intensity &gt; 100),aes(\n    x = mass, \n    y = intensity\n  ),  size = 3, color=\"red\")"
  },
  {
    "objectID": "posts/intro_to_mass_spec/4_reanalysis/index.html#plot-an-extracted-ion-chromatogram-eic",
    "href": "posts/intro_to_mass_spec/4_reanalysis/index.html#plot-an-extracted-ion-chromatogram-eic",
    "title": "Part 4: Introduction to Analysis and Plotting Mass Spectrometry Data in R",
    "section": "Plot an extracted ion chromatogram (EIC)",
    "text": "Plot an extracted ion chromatogram (EIC)\nLet‚Äôs create a extracted ion chromatogram (EIC) for the ‚Äú743.5622 m/z‚Äù precursor.\nTo do that we need to loop through all the MS1 spectra and extract the intensity (in this case the max) of data points within an m/z range.\n\nfull_spectra_header &lt;- header(profile_spectra_handle)\nms1_indices &lt;- full_spectra_header[full_spectra_header$msLevel == 1, ]$seqNum\n\ntarget_mass &lt;- 743.5646\ndelta &lt;- 0.01\n\nleft_window &lt;- target_mass - delta\nright_window &lt;- target_mass + delta\n\n# The lapply below creates a list of two-column data frames for each scan, like:\n#   ret_time intensity\n#   1163.068 0\nlist_of_data_frames  &lt;- lapply(ms1_indices, \n  function(x){\n    ret_time &lt;- full_spectra_header[x, ]$retentionTime\n    x &lt;- mzR::spectra(profile_spectra_handle, x)\n    x &lt;- as.data.frame(x)\n    colnames(x) &lt;- c(\"mass\", \"intensity\")\n    x &lt;- x[x$mass &gt; left_window & x$mass &lt; right_window,  ]\n    if (nrow(x) &gt; 0){\n      return(data.frame(list(ret_time=ret_time, intensity=max(x$intensity))))\n    } else {\n      return(data.frame(list(ret_time=ret_time, intensity=0)))\n    }\n  }\n)\n# concatenate all the data frames together into a single data frame\neic_df &lt;- do.call(\"rbind\", list_of_data_frames)\nremove(list_of_data_frames)\n\nCorresponds to Figure S5E:\n\ntitle = paste(\n  \"Extracted Ion Chromatogram: \",\n  target_mass,\n  \" \",\n  expression(italic(\"m/z\")),\n  \" +/- \",\n  delta,\n   \" Da\")\n\nggplot(\n  data = eic_df,\n  aes(\n    x = ret_time / 60,\n    y = intensity\n  )\n) + \n  geom_line(color=\"gray48\") +\n  xlab(\"Retention Time (min)\") +\n      ggtitle(bquote(\"Extracted Ion Chromatogram:\"~.(target_mass) ~italic(\"m/z\")~\"+/-\"~.(delta) ~\"Da\"))\n\n\n\n\n\n\n\n\nLet‚Äôs plot red circles where the instrument fragmented parent ions between 743 m/z & 745 m/z\n\nggplot(\n  data = eic_df,\n  aes(\n    x = ret_time / 60,\n    y = intensity\n  )\n) + \n  geom_line(color=\"gray48\") + \n  geom_point(\n    data = subset(full_spectra_header, precursorMZ &gt; 743 & precursorMZ &lt; 745),\n    aes(x=retentionTime / 60, y= 5e5),\n    color=\"red\"\n    ) +\n  xlab(\"Retention Time (min)\")\n\n\n\n\n\n\n\n\nAnd we can look at that same information in table form to make sure points weren‚Äôt masked by plotting over each other.\n\nsubset(full_spectra_header, precursorMZ &gt; 743 & precursorMZ &lt; 745)\n\n\n  \n\n\n\nAnother consideration for the experimentalist (and a good exam question üòõ ) is how you could obtain more scans of this 743.5622 m/z target ion. There‚Äôs multiple ways, with the most obvious being to run in targeted mode where you only fragment parent molecules within a tight range around 743.5622 m/z. But if you need untargeted MS you can mess with duty cycles, or adjust your chromatography to increase the elution peak width of the target compound; sometimes 5 minute chromatography isn‚Äôt the best chromatography for the job."
  },
  {
    "objectID": "posts/intro_to_mass_spec/4_reanalysis/index.html#plot-a-centroid-spectrum",
    "href": "posts/intro_to_mass_spec/4_reanalysis/index.html#plot-a-centroid-spectrum",
    "title": "Part 4: Introduction to Analysis and Plotting Mass Spectrometry Data in R",
    "section": "Plot a centroid spectrum",
    "text": "Plot a centroid spectrum\nA centroid spectrum is a bit harder to plot than profile because a simple line won‚Äôt work. For example, look at this ugly mess (circles represent the data points):\n\ncentroid_spectrum &lt;- as.data.table(mzR::peaks(centroid_msfile_handle, 2243))\ncolnames(centroid_spectrum) &lt;- c(\"mass\", \"intensity\")\n\n\nplot(centroid_spectrum, type=\"b\", pch = 16)\n\n\n\n\n\n\n\n\nAnd with ggplot/plotly‚Ä¶\n\np &lt;- ggplot(\n            data=centroid_spectrum,\n            aes(\n              x = mass,\n              y = intensity\n              )\n            ) + \n          geom_line() +\n          geom_point()  + \n          xlab(\"m/z\") + \n          ylab(\"Intensity\")\n\nggplotly(p)\n\n\n\n\n\nLet‚Äôs write a function that will use ggplot and plot each data point as a vertical rectangles.\n\ncentroid_plot &lt;- function(df){\n      df$x1 = df$mass -.2\n      df$x2 = df$mass +.2\n      df$y1 = 0\n      df$y2 = df$intensity\n      df2=df\n      df2$mass &lt;- NULL\n      df2$intensity &lt;- NULL\n\n      p=ggplot() + \n          scale_x_continuous(name=\"m/z\") + \n          scale_y_continuous(name=\"Intensity\") +\n          geom_rect(data=df2, mapping=aes(xmin=x1, xmax=x2, ymin=y1, ymax=y2), color=\"black\", alpha=0.5) +\n          geom_point(data=df2,aes(x = x1,\n                         y = y2,\n                         text = paste(round(x1 +0.2, 2), y2)),\n                     color='transparent')\n\n      return(p)\n  }\n\n\ndf &lt;- as.data.table(mzR::peaks(centroid_msfile_handle, 2243 ))\ncolnames(df) &lt;- c(\"mass\", \"intensity\")\n\ncentroid_plot(df)\n\nWarning in geom_point(data = df2, aes(x = x1, y = y2, text = paste(round(x1 + :\nIgnoring unknown aesthetics: text\n\n\n\n\n\n\n\n\n\nAnd we can pass the ggplot output directly to ggplotly to make the plot interactive:\n\ndf &lt;- as.data.table(mzR::peaks(centroid_msfile_handle, 2243))\ncolnames(df) &lt;- c(\"mass\", \"intensity\")\n\nggplotly(centroid_plot(df))\n\nWarning in geom_point(data = df2, aes(x = x1, y = y2, text = paste(round(x1 + :\nIgnoring unknown aesthetics: text"
  },
  {
    "objectID": "posts/intro_to_mass_spec/4_reanalysis/index.html#plot-mirror-spectra",
    "href": "posts/intro_to_mass_spec/4_reanalysis/index.html#plot-mirror-spectra",
    "title": "Part 4: Introduction to Analysis and Plotting Mass Spectrometry Data in R",
    "section": "Plot mirror spectra",
    "text": "Plot mirror spectra\nRectangles were widened here so the would be easier to see on this blog.\n\ntransform_df &lt;- function(df){\n    df$x1 = df$mass - 0.75\n    df$x2 = df$mass + 0.75\n    df$y1 = 0\n    df$y2 = df$intensity\n    df2=df\n    df2$mass &lt;- NULL\n    df2$intensity &lt;- NULL\n    return(df2)\n}\n    \ncentroid_plot &lt;- function(df1, df2, top_color=\"red\", bottom_color=\"blue\"){\n    df2$intensity &lt;- -df2$intensity\n    df1 &lt;- transform_df(df1)\n    df2 &lt;- transform_df(df2)    \n    \n    p &lt;- ggplot() + \n        scale_x_continuous(name=\"m/z\") + \n        scale_y_continuous(name=\"Intensity\") +\n        geom_rect(data=df1, mapping=aes(xmin=x1, xmax=x2, ymin=y1, ymax=y2), color=\"grey30\", fill=top_color, alpha=0.5) +\n        geom_rect(data=df2, mapping=aes(xmin=x1, xmax=x2, ymin=y1, ymax=y2), fill=bottom_color, color=\"grey30\", alpha=0.5) \n\n    return(p)\n    \n    }\n\n\ndf &lt;- as.data.table(mzR::peaks(centroid_msfile_handle, 2243))\ncolnames(df) &lt;- c(\"mass\", \"intensity\")\n\nggplotly(centroid_plot(df, df))\n\n\n\n\n\n\nCompare with a spectrum in the GNPS library\nDownload GNPS library spectrum CCMSLIB00000072054 (Acyl desferrioxamine C14).\n\ntemporary_directory &lt;- tempdir()\n\n# 22.3 MB\ngnps_spectrum_df &lt;- read.delim(\"https://metabolomics-usi.gnps2.org/csv/?usi1=mzspec%3AGNPS%3AGNPS-LIBRARY%3Aaccession%3ACCMSLIB00000072054\", sep=\",\")\ngnps_spectrum_df &lt;- as.data.table(gnps_spectrum_df)\ncolnames(gnps_spectrum_df) &lt;- c(\"mass\", \"intensity\")\n\nOne important step that I haven‚Äôt discussed yet is normalizing intensity values. This is especially important when comparing data from different sources, where intensity levels may be drastically different. For example, if we just plot our spectrum (positive) against the GNPS library spectrum (negative), we get this:\n\nggplotly(centroid_plot(df, gnps_spectrum_df))\n\n\n\n\n\nBut if we normalize the intensity values of both spectra to a scale of 0 to 100 then we get a useful representation.\n\ndf$intensity &lt;- df$intensity / max(df$intensity) * 100\ngnps_spectrum_df$intensity &lt;- gnps_spectrum_df$intensity / max(gnps_spectrum_df$intensity) * 100\n\n\nggplotly(centroid_plot(df, gnps_spectrum_df))"
  },
  {
    "objectID": "posts/intro_to_mass_spec/3_spectra/index.html",
    "href": "posts/intro_to_mass_spec/3_spectra/index.html",
    "title": "Part 3: Reading mzXML/mzML into R",
    "section": "",
    "text": "This is a continuation from Part 2."
  },
  {
    "objectID": "posts/intro_to_mass_spec/3_spectra/index.html#set-up-an-session",
    "href": "posts/intro_to_mass_spec/3_spectra/index.html#set-up-an-session",
    "title": "Part 3: Reading mzXML/mzML into R",
    "section": "Set Up an üá∑ session",
    "text": "Set Up an üá∑ session\nThe rest of this tutorial will take place using R.\nHere we will install and then load mzR, a Bioconductor package for parsing mass spectrometry data. Vignette here. For plotting we‚Äôll be using ggplot2 and plotly.\n\nif (!require(\"mzR\", quietly = TRUE)){\n  if (!require(\"BiocManager\", quietly = TRUE)){\n      install.packages(\"BiocManager\")\n  }\n  BiocManager::install(\"mzR\")\n}\n  \nlibrary(mzR)"
  },
  {
    "objectID": "posts/intro_to_mass_spec/3_spectra/index.html#download-lc-msms-example-data",
    "href": "posts/intro_to_mass_spec/3_spectra/index.html#download-lc-msms-example-data",
    "title": "Part 3: Reading mzXML/mzML into R",
    "section": "Download LC-MS/MS example data",
    "text": "Download LC-MS/MS example data\nNext let‚Äôs download the LC-MS/MS data we will be working with to a temporary directory (i.e.¬†the directory will be deleted upon closing the R session).\nThere are two files:\n\nan mzXML file which contains centroid data (peaks only)\nan mzML file which contains profile data (raw data,not peak-picked)\n\nGNPS used to require mzXML so that‚Äôs the reason for both mzXML and mzML formats.\nWarning: This is a 22 MB and 306 MB download.\n\n# I have slow internet so I'll increase the amount of time the download is allowed to take\noptions(timeout=240)\n\ntemporary_directory &lt;- tempdir()\n\n# 22.3 MB\npeaks_file_path &lt;- file.path(temporary_directory, \"B022.mzXML\" )\ndownload.file(url = \"ftp://massive.ucsd.edu/v01/MSV000081555/peak/B022.mzXML\",\n             destfile = peaks_file_path)\n             \n# 306.1 MB\nraw_mzml_path &lt;- file.path(temporary_directory, \"B022.mzML\" )\ndownload.file(url = \"ftp://massive.ucsd.edu/v01/MSV000081555/raw/FullSpectra-mzML/B022_GenbankAccession-KY858245.mzML\",\n              destfile = raw_mzml_path)"
  },
  {
    "objectID": "posts/intro_to_mass_spec/3_spectra/index.html#how-to-open-mzxmlmzml-data-in-r",
    "href": "posts/intro_to_mass_spec/3_spectra/index.html#how-to-open-mzxmlmzml-data-in-r",
    "title": "Part 3: Reading mzXML/mzML into R",
    "section": "How to open mzXML/mzML data in R",
    "text": "How to open mzXML/mzML data in R\nIf we look at the first ten lines of:\n\npeaks_file_path\n\n[1] \"/tmp/RtmpHtTeyr/B022.mzXML\"\n\n\nWe can see it is indeed an mzXML file:\n\ncat(readLines(peaks_file_path, n=10), sep = \"\\n\")\n\n&lt;?xml version=\"1.0\" encoding=\"ISO-8859-1\"?&gt;\n&lt;mzXML xmlns=\"http://sashimi.sourceforge.net/schema_revision/mzXML_3.2\"\n       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n       xsi:schemaLocation=\"http://sashimi.sourceforge.net/schema_revision/mzXML_3.2 http://sashimi.sourceforge.net/schema_revision/mzXML_3.2/mzXML_idx_3.2.xsd\"&gt;\n  &lt;msRun scanCount=\"4399\" startTime=\"PT0.0673789S\" endTime=\"PT1200.11S\"&gt;\n    &lt;parentFile fileName=\"file:///C:\\Users\\chase\\Downloads\\LCMSNORTHWESTERN\\Example\\Input_Folder/20170719_mwm1013_metabologenomics_actinolunaC182x100_B022.raw\"\n                fileType=\"RAWData\"\n                fileSha1=\"b739a75b1c680e889940f7b35fe9ef07ee5bcd62\"/&gt;\n    &lt;msInstrument msInstrumentID=\"1\"&gt;\n      &lt;msManufacturer category=\"msManufacturer\" value=\"Thermo Scientific\"/&gt;\n\n\nI‚Äôm a fan of mzR due to its speed (under the hood is a lot of fast C++ code), and that it lazily loads the data from mzXML/mzML files (it doesn‚Äôt read everything into memory unless you request it).\nHere we will tell mzR to lazily open the mass spec file we just downloaded. We can see it returns a handle to the file, which contains 4399 ‚Äúscans‚Äù. A scan being a mass spectrum.\n\nmsfile_handle &lt;- mzR::openMSfile(peaks_file_path)\nmsfile_handle\n\nMass Spectrometry file handle.\nFilename:  B022.mzXML \nNumber of scans:  4399 \n\n\nmzR uses S3 object oriented programming which is difficult if you are only used to R‚Äôs usual functional programming style. You don‚Äôt have to worry much about it because most of what I‚Äôll show is functional, but if you do care there are a number of object based methods you can use.\nWe can see how mzR ‚Äúopened/parsed‚Äù the file, here using C++ code from ProteoWizard.\n\nmsfile_handle@backend\n\nC++ object &lt;0x5624ada3aa70&gt; of class 'Pwiz' &lt;0x5624aecbea90&gt;\n\n\nOne of the most useful {mzR} functions is header() which provides summarizing information about each scan in the dataset. Each scan is numbered sequentially (seqNum/acquisitionNum).\n\nsummary_data &lt;- header(msfile_handle)\nhead(summary_data, 5)\n\n\n  \n\n\n\nTHis allows us to do things like filtering for only positive mode MS2 scans.\n\nfiltered_df &lt;- summary_data[summary_data$polarity == 1, ][summary_data$msLevel == 2, ]\nhead(filtered_df, 5)\n\n\n  \n\n\n\nThe other useful function I‚Äôll bring up in this post retrieves the actual mass spectra. If just provided the file handle it will load every scan in the file as a separate two-column matrix. For each matrix the first column represents m/z and the second column is intensity. If a scan number is provided it will return the two-column matrix for that specific scan.\nLet‚Äôs look at the first five lines of the fourth scan/mass spectrum.\n\n# note: mzR::peaks() and mzR::spectra() are interchangeable\nsingle_spectrum &lt;- mzR::peaks(msfile_handle, scans=4)\nhead(single_spectrum, 5)\n\n           mz intensity\n[1,] 150.0265 32913.336\n[2,] 151.0238  2110.815\n[3,] 151.0272  3636.793\n[4,] 152.0564  4872.385\n[5,] 153.0907  2387.040"
  },
  {
    "objectID": "posts/intro_to_mass_spec/3_spectra/index.html#check-our-assumptions",
    "href": "posts/intro_to_mass_spec/3_spectra/index.html#check-our-assumptions",
    "title": "Part 3: Reading mzXML/mzML into R",
    "section": "Check our assumptions",
    "text": "Check our assumptions\nWhenever you get new data the first thing you should do is get a feel for the data and confirm any assumptions that could influence your analysis. Some, but not all of the ways you can do so are shown here.\nWe can use mzR::openMSfile() to open the file in R.\n\nfull_spectra_handle &lt;- mzR::openMSfile(raw_mzml_path)\n\nUsing the header() function we can peak at the first 10 rows of the summary information about each spectrum in the file. This is not reading any spectra data yet but pulling metadata about each spectrum that is stored within the mzML file, as described in post 2. We can see there is both MS1 and MS2 spectra i this file.\n\nAre the number of data points for the MS1 and MS2 spectra within the range I expected? (peaksCount column)\n\nheader_table &lt;- header(full_spectra_handle)\nhead(header_table, 10)\n\n\n  \n\n\n\n\n\nIs this centroid or profile data?\nAll the values in header_table$centroided are false so all the scans in the file are profile. This aligns with the large numbers seen in the peaksCount column above.\n\ntable(header_table$centroided)\n\n\nFALSE \n 4399 \n\n\n\n\nWhat max intensities can I expect?\n\nsummary(header_table$totIonCurrent)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n    12168   1841128  10199948  14827967  19327535 341675010 \n\n\n\n\nWhat are the min/max retention times?\n\npaste0(\"Minimum retention time: \", min(header_table$retentionTime), \" s\")\n\n[1] \"Minimum retention time: 0.06737892 s\"\n\npaste0(\"Maximum retention time: \", max(header_table$retentionTime) , \" s\")\n\n[1] \"Maximum retention time: 1200.10806 s\"\n\n\n\n\nDoes the data contain positive or negative mode spectra? Both?\nHere it‚Äôs positive because the only value within the polarity column is 1\n\ntable(header_table$polarity)\n\n\n   1 \n4399 \n\n\n\n\nHow many MS1 MS2 scans are there?\n\ntable(header_table$msLevel)\n\n\n   1    2 \n3283 1116 \n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nIt should be noted that the peaksCount column has the same name whether you have loaded centroid or profile data and is best thought of as the number of data points within a single scan/spectrum."
  },
  {
    "objectID": "posts/intro_to_python_and_jupyter/index.html",
    "href": "posts/intro_to_python_and_jupyter/index.html",
    "title": "Intro to Python and Jupyter",
    "section": "",
    "text": "When people talk about processing scientific data, they rarely mean hitting a button on an automated system and waiting for a finished result. As you work through your hypothesis, you may often find that your question changes based on how the data come together. This is true of scientific coding as well. In practice, you will constantly be designing functions and need to evaluate how well they work - or troubleshoot why it does not run at all.\nThe majority of the tutorials we will be sharing will be written in Python - but that doesn‚Äôt have to mean that we need to write a python script from scratch. What you are now reading is written in a Jupyter Notebook - a dynamic environment capabile of both processing code, and displaying results in a step-wise fashon. A Jupyter notebook is segmented, allowing you to write a specfic set of instructions in each cell and executing it to see the results without needing to re-run the entire script. From a practical perspective, you can think of a Jupyter notebook as a rapid prototyping sandbox. Once you have code that works as you expect, you can design larger applications that use the scripts you construct to automate a large protion of your processing tasks.\n\n\n\nIn this notebook, we will cover: - The basics of coding in python - How to read in your data - Python packages (and why you should use them) - Designing your own functions (and why you‚Äôll need to)\n\n\n\nWe will search data from the NPAtlas using APIs to get information about a set of compounds by using a customized function.\n\n\n\nPython is an incredibly flexible programming language that allows you to design solutions to problems very quickly. Unlike more complex coding languages, you can create variables ‚Äúon the fly‚Äù as you need them without declaring them at the very begining of the script. This allows you to pass data through these variables to other processes in a way that can be read easily by other processes. You may hear that python is ‚Äúslow‚Äù, but speed is rarely the main focus of designing a new tool in python. Because of it‚Äôs simple structure and easy readability - it is often the go-to language for scientific programming.\n\n\n\nPython has some in-built structures for data that we will be using to store, sort, and manipulate our data.\n\n\nThe most basic data types we interact with in python are strings, integers, and floats. - Strings can be thought of as text and are surrounded by quotations so that python knows this is the kind of data we mean to provide. - Integers are whole numbers, and can be declared directly - Floats are numbers with decimals.\n\n\n\nVariables can be thought of the same in coding as they are in math, physics, or chemistry. Here, we can ascribe an attribute to a variable and call upon it later. Run the cell below by hitting shift+enter to execute the code outlined in the next block.\n\nanswer = 42\nprecise_answer = 42.0\nname = \"Deep Thought\"\n\nIn the cell above, we declared three new variables using each of the three data types we discussed above. Now, we can recall them at any time by simply declaring them. We can manipulate the data each of these variables contain, or use combinations of these variables to arrive at an answer to a question.\n\n\n\nPython contains a number of helpful functions that allow you perform all kinds of tasks and view the results. Basic math functions can be done directly, and you can view the answer by using the in-built print() function. There are also tricks that you can use with functions such as combining variables and text in the print function.\n\nprint(answer)\nprint(precise_answer)\nprint(name, \"says the answer is\", answer)\n\n42\n42.0\nDeep Thought says the answer is 42\n\n\n\n\n\nThe print function is one of python‚Äôs simplist functions and allows you to view any variable you‚Äôve declared. In a jupyter notebook, you can call on a variable directly to see what it contains - but this is not something that can be done in python directly.\n\nanswer\n\n42\n\n\nIf we wanted to change the answer, we can perform an operation on it:\n\nanswer - 24\n\n18\n\n\nBe warned! We didn‚Äôt tell the system to store the new answer as a variable - so it won‚Äôt remember what the new answer actually is unless we tell it to remember:\n\nprint(\"The answer is still:\",answer)\n\nnew_answer = answer - 24\n\nprint(\"But the new answer is\",new_answer)\n\nThe answer is still: 42\nBut the new answer is 18\n\n\nIf you want to see if one value is equal to another, we use two equals signs to tell python to evaluate a statement, rather than declaring a variable. In the example below, if the statement is true, then we will get the answer True - however, we are expecting it to say False.\n\nanswer == new_answer\n\nFalse\n\n\nThis is very handy for checking to see if something satisfies some conditions we have for data filtration. We can also ask python if the values are NOT equal by using the following:\n\nanswer != new_answer\n\nTrue\n\n\n\n\n\nLists are collections of values or variables that you can use to store information. They can contain strings, integers, floats, and even complex objects - such as other lists. We declare lists with square brackets [] and separate elements of a list with a comma.\n\nbacteria_genera = [\"Escherichia\", \"Salmonella\", \"Bacillus\", \"Staphylococcus\", \"Streptococcus\",\"Bhurkholderia\"]\n\nLists can be manipulated directly - you can add or remove items adding .append() or .remove() to the name of the list. You can also fetch specific values in a list by referencing their location. In most data science areas, we start an index at position zero, so to fetch the second value in a list, we need to tell it to fetch the position at 1, not 2.\n\nbacteria_genera.append(\"Clostridium\")\nprint(bacteria_genera)\nbacteria_genera.remove('Streptococcus')\nprint(bacteria_genera)\nprint('The second entry in our list is:',bacteria_genera[1])\n\n['Escherichia', 'Salmonella', 'Bacillus', 'Staphylococcus', 'Streptococcus', 'Bhurkholderia', 'Clostridium']\n['Escherichia', 'Salmonella', 'Bacillus', 'Staphylococcus', 'Bhurkholderia', 'Clostridium']\nThe second entry in our list is: Salmonella\n\n\nIf you wanted to interact with a certain element of the list, you can do that by referring to its place in the list (numerically)\n\nprint(\"The first item in the bacteria_genera list is:\",bacteria_genera[0])\nprint(\"The last item in the bacteria_genera list is:\",bacteria_genera[-1])\n\nThe first item in the bacteria_genera list is: Escherichia\nThe last item in the bacteria_genera list is: Clostridium\n\n\nIf you wanted to go through every item in the list, you can create a ‚Äúfor loop‚Äù to do that. This becomes very handy if you want to go through lots of data and do the same thing, and is not limited to lists. In the example below, we use a placeholder of ‚Äògenus‚Äô to hold the information we are getting each time we go through the loop - so it gets overwritten every time it goes through the next item. ‚ÄúFor loops‚Äù are handy, but they can be inefficient in the long run - we‚Äôll handle advanced ways to go through lists in the future.\n\nfor genus in bacteria_genera:\n    print(genus)\n\nEscherichia\nSalmonella\nBacillus\nStaphylococcus\nBhurkholderia\nClostridium\n\n\nSimilar to lists, dictionaries store the values you pass to them - but they are indexed. This means that you can give it a key to remember a value by and quickly retrieve that value by using the key at any time. When creating a dictonary, we provide the keys and values in one step while using the curly brackets to tell python that we are dealing with these data in a dictionary. We use dictionaries to store data for organization and speed of data retrieval. Like using an index in the back of a book - rather than scanning every page - we can quickly find where the data we are looking for is and retrieve it.\n\nisolation_locations = {\"Escherichia\":\"intestine\", \"Salmonella\":\"intestine\", \"Bacillus\":\"soil\", \"Staphylococcus\":\"skin\", \"Streptococcus\":\"throat\",\"Bhurkholderia\":\"soil\"}\nisolation_locations[\"Escherichia\"]  \n\n'intestine'\n\n\nIf you wanted to add a new value to the dictionary, you can do that after it is created by providing a new key and value pair:\n\nisolation_locations[\"Clostridium\"] = \"soil\"\nisolation_locations\n\n{'Escherichia': 'intestine',\n 'Salmonella': 'intestine',\n 'Bacillus': 'soil',\n 'Staphylococcus': 'skin',\n 'Streptococcus': 'throat',\n 'Bhurkholderia': 'soil',\n 'Clostridium': 'soil'}\n\n\nDictionaries and lists can be nested as well, so you can have a list of lists, or a dictionary of dictionaries. One format we will work with by the end of this lesson -JSON- can be manipulated like a dictionary of dictionaries!\n\n\n\n\nLet‚Äôs say we isolated a new genera from our experiments and wanted to update both the list and dictionary. Add a new genera to the list and update the dictionary with its isolation location:\n\n### Exercise 1 Workspace: \n\n\n\n\n\nConstructing a dictonary or list one element at a time can be useful - but very tedious. Often, we have excel spreadsheets or .csv/.tsv data files that contain the kinds of information we want to interact with. If you haven‚Äôt worked with a .tsv file before - it is very similar to a .csv except that each value is separated by a tab instead of a comma. Although excel files are very human readable - they‚Äôre very cumbersome for computation. TSV and CSV files are very efficent, and we can parse them using some in-built python functions. However, because python does not know that we will interact with a tsv or csv file, it has all of the functions we need to use tucked away to be more efficient. We can tell python to import this package so that we can fetch the data.\nWe will use a data file from the NP Atlas to construct a new list of bacteria that are relevant for natural product drug discovery. For that, let‚Äôs focus only on the genera reported for each compound‚Äôs initial discovery:\n\nimport csv \n\nwith open('NPAtlas_download.tsv', 'r') as file:\n    line_reader = csv.reader(file, delimiter='\\t')\n    headers = next(line_reader)\nprint(headers)\n\n['npaid', 'compound_id', 'compound_name', 'compound_molecular_formula', 'compound_molecular_weight', 'compound_accurate_mass', 'compound_m_plus_h', 'compound_m_plus_na', 'compound_inchi', 'compound_inchikey', 'compound_smiles', 'compound_cluster_id', 'compound_node_id', 'origin_type', 'genus', 'origin_species', 'original_reference_author_list', 'original_reference_year', 'original_reference_issue', 'original_reference_volume', 'original_reference_pages', 'original_reference_doi', 'original_reference_pmid', 'original_reference_title', 'original_reference_type', 'original_journal_title', 'synonyms_dois', 'reassignment_dois', 'synthesis_dois', 'mibig_ids', 'gnps_ids', 'cmmc_ids', 'npmrd_id', 'npatlas_url']\n\n\nYou may notice that none of the headers contain spaces. This is because certain types of functions and files do not behave very well with spaces and it is usually best practice to use an underscore instead. For our task, two headers are going to be very important: ‚Äúorigin_type‚Äù and ‚Äúgenus‚Äù\nIt is possible to use the csv package to parse all these data and populate a new list of genera that we can focus on. The csv file reader works on a line-by-line basis. It reads the lines we tell it (typically, every line) one at a time, fetches the data, and we can use it. Let‚Äôs construct a list of all of the genera contained in the NPAtlas and print the first 10 entries in the list:\n\nnp_atlas_genera = []\n\nwith open('NPAtlas_download.tsv', 'r') as file:\n    line_reader = csv.reader(file, delimiter='\\t')\n    for each_row in line_reader:\n        np_atlas_genera.append(each_row[14]) #this adds the 15th column of the each record to the list, which is the genus column - the very first column is 0\nprint(np_atlas_genera[0:10])\n\n['genus', 'Curvularia', 'Diaporthe', 'Streptomyces', 'Vibrio', 'Fusarium', 'Microbispora', 'Chaetomium', 'Myxococcus', 'Penicillium']\n\n\nYou will notice that the first entry in the list is our header - ‚Äògenus‚Äô. Since we don‚Äôt care about this, we can filter it out as we construct the list. Also, because a list doesn‚Äôt care about repeated values, we have MANY duplicates in our list. Let‚Äôs filter out duplicates at the end by converting our list to a set(). A set behaves very closely to a list, but you can only have each value in a set once.\n\nnp_atlas_genera = []\n\nwith open('NPAtlas_download.tsv', 'r') as file:\n    line_reader = csv.reader(file, delimiter='\\t')\n    headers = next(line_reader) # read in the column headers so that we know we can skip adding it to the list\n    for each_row in line_reader:\n        if each_row[14] not in headers: # exclude the header value (hint: you can also use )\n            np_atlas_genera.append(each_row[14]) #this adds the 15th column of the each record to the list, which is the genus column - Remember: the very first column is 0\n\nprint('The NPAtlas database contains',len(np_atlas_genera),'genera from bacteria, fungi, and archaea.')\n\nnp_atlas_set = set(np_atlas_genera)\n\nprint('After removing duplicate values, there are',len(np_atlas_set),'unique genera in the NPAtlas dataset.')\n\nThe NPAtlas database contains 36454 genera from bacteria, fungi, and archaea.\nAfter removing duplicate values, there are 1246 unique genera in the NPAtlas dataset.\n\n\n\n\n\nIf we only wanted the genera of bacteria to be added to our list of bacteria_genera - how can you alter the function above?\n\n## Exercise 2 Workspace:\n\nTo see the solution to this excercise, remove the # at the begining of the next line and run the cell.\n\n# %load ./exercise_solutions/exercise_2.py"
  },
  {
    "objectID": "posts/intro_to_python_and_jupyter/index.html#what-is-a-jupyter-notebook",
    "href": "posts/intro_to_python_and_jupyter/index.html#what-is-a-jupyter-notebook",
    "title": "Intro to Python and Jupyter",
    "section": "",
    "text": "When people talk about processing scientific data, they rarely mean hitting a button on an automated system and waiting for a finished result. As you work through your hypothesis, you may often find that your question changes based on how the data come together. This is true of scientific coding as well. In practice, you will constantly be designing functions and need to evaluate how well they work - or troubleshoot why it does not run at all.\nThe majority of the tutorials we will be sharing will be written in Python - but that doesn‚Äôt have to mean that we need to write a python script from scratch. What you are now reading is written in a Jupyter Notebook - a dynamic environment capabile of both processing code, and displaying results in a step-wise fashon. A Jupyter notebook is segmented, allowing you to write a specfic set of instructions in each cell and executing it to see the results without needing to re-run the entire script. From a practical perspective, you can think of a Jupyter notebook as a rapid prototyping sandbox. Once you have code that works as you expect, you can design larger applications that use the scripts you construct to automate a large protion of your processing tasks."
  },
  {
    "objectID": "posts/intro_to_python_and_jupyter/index.html#lesson-objectives",
    "href": "posts/intro_to_python_and_jupyter/index.html#lesson-objectives",
    "title": "Intro to Python and Jupyter",
    "section": "",
    "text": "In this notebook, we will cover: - The basics of coding in python - How to read in your data - Python packages (and why you should use them) - Designing your own functions (and why you‚Äôll need to)"
  },
  {
    "objectID": "posts/intro_to_python_and_jupyter/index.html#lesson-case-study",
    "href": "posts/intro_to_python_and_jupyter/index.html#lesson-case-study",
    "title": "Intro to Python and Jupyter",
    "section": "",
    "text": "We will search data from the NPAtlas using APIs to get information about a set of compounds by using a customized function."
  },
  {
    "objectID": "posts/intro_to_python_and_jupyter/index.html#why-python",
    "href": "posts/intro_to_python_and_jupyter/index.html#why-python",
    "title": "Intro to Python and Jupyter",
    "section": "",
    "text": "Python is an incredibly flexible programming language that allows you to design solutions to problems very quickly. Unlike more complex coding languages, you can create variables ‚Äúon the fly‚Äù as you need them without declaring them at the very begining of the script. This allows you to pass data through these variables to other processes in a way that can be read easily by other processes. You may hear that python is ‚Äúslow‚Äù, but speed is rarely the main focus of designing a new tool in python. Because of it‚Äôs simple structure and easy readability - it is often the go-to language for scientific programming."
  },
  {
    "objectID": "posts/intro_to_python_and_jupyter/index.html#python-data-structures",
    "href": "posts/intro_to_python_and_jupyter/index.html#python-data-structures",
    "title": "Intro to Python and Jupyter",
    "section": "",
    "text": "Python has some in-built structures for data that we will be using to store, sort, and manipulate our data.\n\n\nThe most basic data types we interact with in python are strings, integers, and floats. - Strings can be thought of as text and are surrounded by quotations so that python knows this is the kind of data we mean to provide. - Integers are whole numbers, and can be declared directly - Floats are numbers with decimals.\n\n\n\nVariables can be thought of the same in coding as they are in math, physics, or chemistry. Here, we can ascribe an attribute to a variable and call upon it later. Run the cell below by hitting shift+enter to execute the code outlined in the next block.\n\nanswer = 42\nprecise_answer = 42.0\nname = \"Deep Thought\"\n\nIn the cell above, we declared three new variables using each of the three data types we discussed above. Now, we can recall them at any time by simply declaring them. We can manipulate the data each of these variables contain, or use combinations of these variables to arrive at an answer to a question.\n\n\n\nPython contains a number of helpful functions that allow you perform all kinds of tasks and view the results. Basic math functions can be done directly, and you can view the answer by using the in-built print() function. There are also tricks that you can use with functions such as combining variables and text in the print function.\n\nprint(answer)\nprint(precise_answer)\nprint(name, \"says the answer is\", answer)\n\n42\n42.0\nDeep Thought says the answer is 42\n\n\n\n\n\nThe print function is one of python‚Äôs simplist functions and allows you to view any variable you‚Äôve declared. In a jupyter notebook, you can call on a variable directly to see what it contains - but this is not something that can be done in python directly.\n\nanswer\n\n42\n\n\nIf we wanted to change the answer, we can perform an operation on it:\n\nanswer - 24\n\n18\n\n\nBe warned! We didn‚Äôt tell the system to store the new answer as a variable - so it won‚Äôt remember what the new answer actually is unless we tell it to remember:\n\nprint(\"The answer is still:\",answer)\n\nnew_answer = answer - 24\n\nprint(\"But the new answer is\",new_answer)\n\nThe answer is still: 42\nBut the new answer is 18\n\n\nIf you want to see if one value is equal to another, we use two equals signs to tell python to evaluate a statement, rather than declaring a variable. In the example below, if the statement is true, then we will get the answer True - however, we are expecting it to say False.\n\nanswer == new_answer\n\nFalse\n\n\nThis is very handy for checking to see if something satisfies some conditions we have for data filtration. We can also ask python if the values are NOT equal by using the following:\n\nanswer != new_answer\n\nTrue\n\n\n\n\n\nLists are collections of values or variables that you can use to store information. They can contain strings, integers, floats, and even complex objects - such as other lists. We declare lists with square brackets [] and separate elements of a list with a comma.\n\nbacteria_genera = [\"Escherichia\", \"Salmonella\", \"Bacillus\", \"Staphylococcus\", \"Streptococcus\",\"Bhurkholderia\"]\n\nLists can be manipulated directly - you can add or remove items adding .append() or .remove() to the name of the list. You can also fetch specific values in a list by referencing their location. In most data science areas, we start an index at position zero, so to fetch the second value in a list, we need to tell it to fetch the position at 1, not 2.\n\nbacteria_genera.append(\"Clostridium\")\nprint(bacteria_genera)\nbacteria_genera.remove('Streptococcus')\nprint(bacteria_genera)\nprint('The second entry in our list is:',bacteria_genera[1])\n\n['Escherichia', 'Salmonella', 'Bacillus', 'Staphylococcus', 'Streptococcus', 'Bhurkholderia', 'Clostridium']\n['Escherichia', 'Salmonella', 'Bacillus', 'Staphylococcus', 'Bhurkholderia', 'Clostridium']\nThe second entry in our list is: Salmonella\n\n\nIf you wanted to interact with a certain element of the list, you can do that by referring to its place in the list (numerically)\n\nprint(\"The first item in the bacteria_genera list is:\",bacteria_genera[0])\nprint(\"The last item in the bacteria_genera list is:\",bacteria_genera[-1])\n\nThe first item in the bacteria_genera list is: Escherichia\nThe last item in the bacteria_genera list is: Clostridium\n\n\nIf you wanted to go through every item in the list, you can create a ‚Äúfor loop‚Äù to do that. This becomes very handy if you want to go through lots of data and do the same thing, and is not limited to lists. In the example below, we use a placeholder of ‚Äògenus‚Äô to hold the information we are getting each time we go through the loop - so it gets overwritten every time it goes through the next item. ‚ÄúFor loops‚Äù are handy, but they can be inefficient in the long run - we‚Äôll handle advanced ways to go through lists in the future.\n\nfor genus in bacteria_genera:\n    print(genus)\n\nEscherichia\nSalmonella\nBacillus\nStaphylococcus\nBhurkholderia\nClostridium\n\n\nSimilar to lists, dictionaries store the values you pass to them - but they are indexed. This means that you can give it a key to remember a value by and quickly retrieve that value by using the key at any time. When creating a dictonary, we provide the keys and values in one step while using the curly brackets to tell python that we are dealing with these data in a dictionary. We use dictionaries to store data for organization and speed of data retrieval. Like using an index in the back of a book - rather than scanning every page - we can quickly find where the data we are looking for is and retrieve it.\n\nisolation_locations = {\"Escherichia\":\"intestine\", \"Salmonella\":\"intestine\", \"Bacillus\":\"soil\", \"Staphylococcus\":\"skin\", \"Streptococcus\":\"throat\",\"Bhurkholderia\":\"soil\"}\nisolation_locations[\"Escherichia\"]  \n\n'intestine'\n\n\nIf you wanted to add a new value to the dictionary, you can do that after it is created by providing a new key and value pair:\n\nisolation_locations[\"Clostridium\"] = \"soil\"\nisolation_locations\n\n{'Escherichia': 'intestine',\n 'Salmonella': 'intestine',\n 'Bacillus': 'soil',\n 'Staphylococcus': 'skin',\n 'Streptococcus': 'throat',\n 'Bhurkholderia': 'soil',\n 'Clostridium': 'soil'}\n\n\nDictionaries and lists can be nested as well, so you can have a list of lists, or a dictionary of dictionaries. One format we will work with by the end of this lesson -JSON- can be manipulated like a dictionary of dictionaries!"
  },
  {
    "objectID": "posts/intro_to_python_and_jupyter/index.html#exercise-1",
    "href": "posts/intro_to_python_and_jupyter/index.html#exercise-1",
    "title": "Intro to Python and Jupyter",
    "section": "",
    "text": "Let‚Äôs say we isolated a new genera from our experiments and wanted to update both the list and dictionary. Add a new genera to the list and update the dictionary with its isolation location:\n\n### Exercise 1 Workspace:"
  },
  {
    "objectID": "posts/intro_to_python_and_jupyter/index.html#reading-in-data-files",
    "href": "posts/intro_to_python_and_jupyter/index.html#reading-in-data-files",
    "title": "Intro to Python and Jupyter",
    "section": "",
    "text": "Constructing a dictonary or list one element at a time can be useful - but very tedious. Often, we have excel spreadsheets or .csv/.tsv data files that contain the kinds of information we want to interact with. If you haven‚Äôt worked with a .tsv file before - it is very similar to a .csv except that each value is separated by a tab instead of a comma. Although excel files are very human readable - they‚Äôre very cumbersome for computation. TSV and CSV files are very efficent, and we can parse them using some in-built python functions. However, because python does not know that we will interact with a tsv or csv file, it has all of the functions we need to use tucked away to be more efficient. We can tell python to import this package so that we can fetch the data.\nWe will use a data file from the NP Atlas to construct a new list of bacteria that are relevant for natural product drug discovery. For that, let‚Äôs focus only on the genera reported for each compound‚Äôs initial discovery:\n\nimport csv \n\nwith open('NPAtlas_download.tsv', 'r') as file:\n    line_reader = csv.reader(file, delimiter='\\t')\n    headers = next(line_reader)\nprint(headers)\n\n['npaid', 'compound_id', 'compound_name', 'compound_molecular_formula', 'compound_molecular_weight', 'compound_accurate_mass', 'compound_m_plus_h', 'compound_m_plus_na', 'compound_inchi', 'compound_inchikey', 'compound_smiles', 'compound_cluster_id', 'compound_node_id', 'origin_type', 'genus', 'origin_species', 'original_reference_author_list', 'original_reference_year', 'original_reference_issue', 'original_reference_volume', 'original_reference_pages', 'original_reference_doi', 'original_reference_pmid', 'original_reference_title', 'original_reference_type', 'original_journal_title', 'synonyms_dois', 'reassignment_dois', 'synthesis_dois', 'mibig_ids', 'gnps_ids', 'cmmc_ids', 'npmrd_id', 'npatlas_url']\n\n\nYou may notice that none of the headers contain spaces. This is because certain types of functions and files do not behave very well with spaces and it is usually best practice to use an underscore instead. For our task, two headers are going to be very important: ‚Äúorigin_type‚Äù and ‚Äúgenus‚Äù\nIt is possible to use the csv package to parse all these data and populate a new list of genera that we can focus on. The csv file reader works on a line-by-line basis. It reads the lines we tell it (typically, every line) one at a time, fetches the data, and we can use it. Let‚Äôs construct a list of all of the genera contained in the NPAtlas and print the first 10 entries in the list:\n\nnp_atlas_genera = []\n\nwith open('NPAtlas_download.tsv', 'r') as file:\n    line_reader = csv.reader(file, delimiter='\\t')\n    for each_row in line_reader:\n        np_atlas_genera.append(each_row[14]) #this adds the 15th column of the each record to the list, which is the genus column - the very first column is 0\nprint(np_atlas_genera[0:10])\n\n['genus', 'Curvularia', 'Diaporthe', 'Streptomyces', 'Vibrio', 'Fusarium', 'Microbispora', 'Chaetomium', 'Myxococcus', 'Penicillium']\n\n\nYou will notice that the first entry in the list is our header - ‚Äògenus‚Äô. Since we don‚Äôt care about this, we can filter it out as we construct the list. Also, because a list doesn‚Äôt care about repeated values, we have MANY duplicates in our list. Let‚Äôs filter out duplicates at the end by converting our list to a set(). A set behaves very closely to a list, but you can only have each value in a set once.\n\nnp_atlas_genera = []\n\nwith open('NPAtlas_download.tsv', 'r') as file:\n    line_reader = csv.reader(file, delimiter='\\t')\n    headers = next(line_reader) # read in the column headers so that we know we can skip adding it to the list\n    for each_row in line_reader:\n        if each_row[14] not in headers: # exclude the header value (hint: you can also use )\n            np_atlas_genera.append(each_row[14]) #this adds the 15th column of the each record to the list, which is the genus column - Remember: the very first column is 0\n\nprint('The NPAtlas database contains',len(np_atlas_genera),'genera from bacteria, fungi, and archaea.')\n\nnp_atlas_set = set(np_atlas_genera)\n\nprint('After removing duplicate values, there are',len(np_atlas_set),'unique genera in the NPAtlas dataset.')\n\nThe NPAtlas database contains 36454 genera from bacteria, fungi, and archaea.\nAfter removing duplicate values, there are 1246 unique genera in the NPAtlas dataset."
  },
  {
    "objectID": "posts/intro_to_python_and_jupyter/index.html#exercise-2",
    "href": "posts/intro_to_python_and_jupyter/index.html#exercise-2",
    "title": "Intro to Python and Jupyter",
    "section": "",
    "text": "If we only wanted the genera of bacteria to be added to our list of bacteria_genera - how can you alter the function above?\n\n## Exercise 2 Workspace:\n\nTo see the solution to this excercise, remove the # at the begining of the next line and run the cell.\n\n# %load ./exercise_solutions/exercise_2.py"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Pharmacognosy",
    "section": "",
    "text": "Building a PC for informatics!\n\n\n\n\n\n\ncomputing hardware\n\n\nother\n\n\n\nInformation about the parts list, building, and setting up a new PC for bioinformatics.\n\n\n\n\n\nJan 30, 2025\n\n\nChase Clark\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Python and Jupyter\n\n\n\n\n\n\nbeginner\n\n\npython\n\n\n\nA task-based introduction into coding with python in the jupyter notebook. This lession teaches how to interact with tsv files to retrieve data, introduces custom functions, and API interaction.\n\n\n\n\n\nJan 8, 2025\n\n\nJoseph Egan\n\n\n\n\n\n\n\n\n\n\n\n\nChemical similarity with Python and RDKit\n\n\n\n\n\n\nbeginner\n\n\nchemistry\n\n\npython\n\n\n\nA very brief introduction to calculating chemical similarity\n\n\n\n\n\nMar 12, 2024\n\n\nChase Clark\n\n\n\n\n\n\n\n\n\n\n\n\nPart 4: Introduction to Analysis and Plotting Mass Spectrometry Data in R\n\n\n\n\n\n\nbeginner\n\n\nmass spectrometry\n\n\nr\n\n\n\nAn introduction to working with mass spectrometry data in R\n\n\n\n\n\nFeb 4, 2024\n\n\nChase M Clark\n\n\n\n\n\n\n\n\n\n\n\n\nPart 3: Reading mzXML/mzML into R\n\n\n\n\n\n\nbeginner\n\n\nmass spectrometry\n\n\nr\n\n\n\nAn introduction to working with mass spectrometry data in R\n\n\n\n\n\nFeb 3, 2024\n\n\nChase M Clark\n\n\n\n\n\n\n\n\n\n\n\n\nPart 2: Anatomy of an mzXML/mzML file\n\n\n\n\n\n\nbeginner\n\n\nmass spectrometry\n\n\nr\n\n\n\nWhat mass spectrometry data looks like, using R\n\n\n\n\n\nFeb 2, 2024\n\n\nChase M Clark\n\n\n\n\n\n\n\n\n\n\n\n\nPart 1: Mass Spectrometry Data\n\n\n\n\n\n\nbeginner\n\n\nmass spectrometry\n\n\nr\n\n\n\nFirst in a series of introductory posts about working with mass spectrometry data in R\n\n\n\n\n\nFeb 1, 2024\n\n\nChase M Clark\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "contributors/chase/chase.html#summary",
    "href": "contributors/chase/chase.html#summary",
    "title": "Chase",
    "section": "Summary",
    "text": "Summary\nChase is a Postdoctoral Research Associate at the University of Wisconsin-Madison where he was awarded and partially funded by a T32 Fellowship in the Computation and Informatics in Biology and Medicine (CIBM) training program. Chase completed a PhD in Pharmacognosy with Dr.¬†Brian Murphy at the University of Illinois at Chicago where he was awarded and funded by an NIH F31 fellowship\nChase has &gt;10 years of lab and computational experience surrounding natural products. Non-comprehensively, this spans analytical chemistry/biochemistry/microbiology method development and R&D at a small enzyme and probiotic manufacturer, to mass spectrometry method and software development in grad school, to engineering large scale genome mining and metagneomic software for drug discovery.\nChase would like to spend more time outdoors this year‚Äì kayaking, scuba diving, cycling, etc.\nMore info can be found at https://chasemc.github.io.",
    "crumbs": [
      "Chase"
    ]
  },
  {
    "objectID": "contributors/chase/chase.html#education",
    "href": "contributors/chase/chase.html#education",
    "title": "Chase",
    "section": "Education",
    "text": "Education\nUniversity of Illinois at Chicago | Chicago, IL | PhD in Pharmacognosy | 2015 - 2020\nBerry College | Mount Berry, GA | B.S. in Biochemisty | 2008 - 2012",
    "crumbs": [
      "Chase"
    ]
  },
  {
    "objectID": "contributors/chase/chase.html#experience",
    "href": "contributors/chase/chase.html#experience",
    "title": "Chase",
    "section": "Experience",
    "text": "Experience\nUniversity of Wisconsin-Madison | Postdoctoral Fellow/Associate (Computational Biology, Data Science, Drug Discovery) | 2020 - present\nDeerland Probiotics & Enzymes | R&D Tech | 2013 - 2015\n(image created with DALL¬∑E 3)\n\nPublications\n\n\n(1) Mullowney, M. W.; Duncan, K. R.; Elsayed, S. S.; Garg, N.; Hooft, J. J. J. van der; Martin, N. I.; Meijer, D.; Terlouw, B. R.; Biermann, F.; Blin, K.; Durairaj, J.; Gorostiola Gonz√°lez, M.; Helfrich, E. J. N.; Huber, F.; Leopold-Messer, S.; Rajan, K.; Rond, T. de; Santen, J. A. van; Sorokina, M.; Balunas, M. J.; Beniddir, M. A.; Bergeijk, D. A. van; Carroll, L. M.; Clark, C. M.; Clevert, D.-A.; Dejong, C. A.; Du, C.; Ferrinho, S.; Grisoni, F.; Hofstetter, A.; Jespers, W.; Kalinina, O. V.; Kautsar, S. A.; Kim, H.; Leao, T. F.; Masschelein, J.; Rees, E. R.; Reher, R.; Reker, D.; Schwaller, P.; Segler, M.; Skinnider, M. A.; Walker, A. S.; Willighagen, E. L.; Zdrazil, B.; Ziemert, N.; Goss, R. J. M.; Guyomard, P.; Volkamer, A.; Gerwick, W. H.; Kim, H. U.; M√ºller, R.; Wezel, G. P. van; Westen, G. J. P. van; Hirsch, A. K. H.; Linington, R. G.; Robinson, S. L.; Medema, M. H. Artificial Intelligence for Natural Product Drug Discovery. Nat. Rev. Drug Discov. 2023, 22 (11), 895‚Äì916. https://doi.org/10.1038/s41573-023-00774-7.\n\n\n(2) Rees, E. R.; Uppal, S.; Clark, C. M.; Lail, A. J.; Waterworth, S. C.; Roesemann, S. D.; Wolf, K. A.; Kwan, J. C. Autometa 2: A Versatile Tool for Recovering Genomes from Highly-Complex Metagenomic Communities. bioRxiv, 2023, 2023.09.01.555939. https://doi.org/10.1101/2023.09.01.555939.\n\n\n(3) Waterworth, S. C.; Rees, E. R.; Clark, C. M.; Carlson, S.; Miller, I. J.; Puglisi, M.; Kwan, J. C. Elevated Expression of Srp RiPPs Across Bacterial Phyla in Marine Sponges. bioRxiv, 2023, 2023.06.09.544420. https://doi.org/10.1101/2023.06.09.544420.\n\n\n(4) Clark, C. M.; Nguyen, L.; Pham, V. C.; Sanchez, L. M.; Murphy, B. T. Automated Microbial Library Generation Using the Bioinformatics Platform IDBac. Molecules 2022, 27 (7). https://doi.org/10.3390/molecules27072038.\n\n\n(5) Clark, C. M.; Hernandez, A.; Mullowney, M. W.; Fitz-Henley, J.; Li, E.; Romanowski, S. B.; Pronzato, R.; Manconi, R.; Sanchez, L. M.; Murphy, B. T. Relationship Between Bacterial Phylotype and Specialized Metabolite Production in the Culturable Microbiome of Two Freshwater Sponges. ISME Communications 2022, 2 (1), 1‚Äì8. https://doi.org/10.1038/s43705-022-00105-8.\n\n\n(6) Elfeki, M.; Mantri, S.; Clark, C. M.; Green, S. J.; Ziemert, N.; Murphy, B. T. Evaluating the Distribution of Bacterial Natural Product Biosynthetic Genes Across Lake Huron Sediment. ACS Chem. Biol. 2021, 16 (11), 2623‚Äì2631. https://doi.org/10.1021/acschembio.1c00653.\n\n\n(7) Leao, T. F.; Clark, C. M.; Bauermeister, A.; Elijah, E. O.; Gentry, E. C.; Husband, M.; Oliveira, M. F.; Bandeira, N.; Wang, M.; Dorrestein, P. C. Quick-Start Infrastructure for Untargeted Metabolomics Analysis in GNPS. Nat Metab 2021, 3 (7), 880‚Äì882. https://doi.org/10.1038/s42255-021-00429-0.\n\n\n(8) Clark, C. M.; Murphy, B. T.; Sanchez, L. M. A Call to Action: The Need for Standardization in Developing Open-Source Mass Spectrometry-Based Methods for Microbial Subspecies Discrimination. mSystems 2020, 5 (1). https://doi.org/10.1128/mSystems.00813-19.\n\n\n(9) Costa, M. S.; Clark, C. M.; √ìmarsd√≥ttir, S.; Sanchez, L. M.; Murphy, B. T. Minimizing Taxonomic and Natural Product Redundancy in Microbial Libraries Using MALDI-TOF MS and the Bioinformatics Pipeline IDBac. J. Nat. Prod. 2019, 82 (8), 2167‚Äì2173. https://doi.org/10.1021/acs.jnatprod.9b00168.\n\n\n(10) Clark, C. M.; Costa, M. S.; Conley, E.; Li, E.; Sanchez, L. M.; Murphy, B. T. Using the Open-Source MALDI TOF-MS IDBac Pipeline for Analysis of Microbial Protein and Specialized Metabolite Data. J. Vis. Exp. 2019, No. 147. https://doi.org/10.3791/59219.\n\n\n(11) Braesel, J.; Clark, C. M.; Kunstman, K. J.; Green, S. J.; Maienschein-Cline, M.; Murphy, B. T.; Eust√°quio, A. S. Genome Sequence of Marine-Derived Streptomyces Sp. Strain F001, a Producer of Akashin a and Diazaquinomycins. Microbiol Resour Announc 2019, 8 (19). https://doi.org/10.1128/MRA.00165-19.\n\n\n(12) Clark, C. M.; Costa, M. S.; Sanchez, L. M.; Murphy, B. T. Coupling MALDI-TOF Mass Spectrometry Protein and Specialized Metabolite Analyses to Rapidly Discriminate Bacterial Function. Proceedings of the National Academy of Sciences, 2018, 115, 4981‚Äì4986. https://doi.org/10.1073/pnas.1801247115.",
    "crumbs": [
      "Chase"
    ]
  },
  {
    "objectID": "about/about.html",
    "href": "about/about.html",
    "title": "About",
    "section": "",
    "text": "This blog was started by two pharmacognosists who realized that our field requires a diverse set of in-demand informatics skills but that there are limited University programs offering detailed instruction on code-based analysis of the wide array of analytical techniques we encounter every day. Here we hope to share some of our own experience in using code to analyze the chemical and biological data a pharmacognosist is likely to encounter in the modern era.\n(image created with DALL¬∑E 3)"
  },
  {
    "objectID": "contributors/joe/joe.html#summary",
    "href": "contributors/joe/joe.html#summary",
    "title": "Joe",
    "section": "Summary",
    "text": "Summary\nJoe is the Director of Product at Ometa Labs LLC, where he works on developing tools for mass spectrometry. Joe recieved his Bachelors and Masters degrees at UNC Greensboro under the training of Dr.¬†Nadja Cech and completed his PhD in Chemistry at Simon Fraser University with Dr.¬†Roger Linington working in NMR Metabolomics. While at SFU, Joe was awarded and funded by the NIH F31 fellowship.\nJoe has worked in natural products for over a decade, working with botanical, fungal, and bacterial sources. He has training in microbial and fungal cultivation, compound isolation and characterization, biological assay development, and metabolomics data analysis. After completing his PhD, Joe moved into software development and machine learning where he applies his previous experience to design user-focused tools for complex datasets.\nJoe enjoys fishing, playing live music, and riding his motorcycle through the Michigan countryside.",
    "crumbs": [
      "Joe"
    ]
  },
  {
    "objectID": "contributors/joe/joe.html#education",
    "href": "contributors/joe/joe.html#education",
    "title": "Joe",
    "section": "Education",
    "text": "Education\nSimon Fraser University | Burnaby, BC, Canada| Ph.D.¬†Chemistry | 2015 - 2021\nUNC Greensboro | Greensboro, NC | B.S. Biochemistry | 2009-2015",
    "crumbs": [
      "Joe"
    ]
  },
  {
    "objectID": "contributors/joe/joe.html#experience",
    "href": "contributors/joe/joe.html#experience",
    "title": "Joe",
    "section": "Experience",
    "text": "Experience\nOmeta Labs LLC | Director of Product | 2021 - present\nData Revenue GmbH | Mass Spectrometry Metabolomics Scientist and Science Writer | 2021-2022\n\nPublications\n\n\n(1) Flores-Bocanegra, L.; Al Subeh, Z. Y.; Egan, J. M.; El-Elimat, T.; Raja, H. A.; Burdette, J. E.; Pearce, C. J.; Linington, R. G.; Oberlies, N. H. Dereplication of Fungal Metabolites by NMR-Based Compound Networking Using MADByTE. Journal of natural products 2022, 85 (3), 614‚Äì624.\n\n\n(2) Egan, J. M.; Santen, J. A. van; Liu, D. Y.; Linington, R. G. Development of an NMR-Based Platform for the Direct Structural Annotation of Complex Natural Products Mixtures. Journal of natural products 2021, 84 (4), 1044‚Äì1055.\n\n\n(3) Brown, A. R.; Ettefagh, K. A.; Todd, D. A.; Cole, P. S.; Egan, J. M.; Foil, D. H.; Lacey, E. P.; Cech, N. B. Bacterial Efflux Inhibitors Are Widely Distributed in Land Plants. Journal of Ethnopharmacology 2021, 267, 113533.\n\n\n(4) Van Santen, J. A.; Jacob, G.; Singh, A. L.; Aniebok, V.; Balunas, M. J.; Bunsko, D.; Neto, F. C.; Casta√±o-Espriu, L.; Chang, C.; Clark, T. N.; others. The Natural Products Atlas: An Open Access Knowledge Base for Microbial Natural Products Discovery. ACS central science 2019, 5 (11), 1824‚Äì1833.\n\n\n(5) McAlpine, J. B.; Chen, S.-N.; Kutateladze, A.; MacMillan, J. B.; Appendino, G.; Barison, A.; Beniddir, M. A.; Biavatti, M. W.; Bluml, S.; Boufridi, A.; others. The Value of Universally Available Raw NMR Data for Transparency, Reproducibility, and Integrity in Natural Product Research. Natural product reports 2019, 36 (1), 35‚Äì107.\n\n\n(6) Kellogg, J. J.; Todd, D. A.; Egan, J. M.; Raja, H. A.; Oberlies, N. H.; Kvalheim, O. M.; Cech, N. B. Biochemometrics for Natural Products Research: Comparison of Data Analysis Approaches and Application to Identification of Bioactive Compounds. Journal of natural products 2016, 79 (2), 376‚Äì386.\n\n\n(7) Egan, J. M.; Kaur, A.; Raja, H. A.; Kellogg, J. J.; Oberlies, N. H.; Cech, N. B. Antimicrobial Fungal Endophytes from the Botanical Medicine Goldenseal (Hydrastis Canadensis). Phytochemistry letters 2016, 17, 219‚Äì225.\n\n\n(8) Brown, A. R.; Ettefagh, K. A.; Todd, D.; Cole, P. S.; Egan, J. M.; Foil, D. H.; Graf, T. N.; Schindler, B. D.; Kaatz, G. W.; Cech, N. B. A Mass Spectrometry-Based Assay for Improved Quantitative Measurements of Efflux Pump Inhibition. PLoS One 2015, 10 (5), e0124814.\n\n\n(9) Bussey III, R. O.; Kaur, A.; Todd, D. A.; Egan, J. M.; El-Elimat, T.; Graf, T. N.; Raja, H. A.; Oberlies, N. H.; Cech, N. B. Comparison of the Chemistry and Diversity of Endophytes Isolated from Wild-Harvested and Greenhouse-Cultivated Yerba Mansa (Anemopsis Californica). Phytochemistry letters 2015, 11, 202‚Äì208.",
    "crumbs": [
      "Joe"
    ]
  },
  {
    "objectID": "posts/building_a_workstation_pc/index.html",
    "href": "posts/building_a_workstation_pc/index.html",
    "title": "Building a PC for informatics!",
    "section": "",
    "text": "Disclaimer: I recently left UW-Madison and am now the owner of Evoquant LLC. Currently we do consulting/freelance bioinformatics work but also have an SBIR submitted in the area of drug discovery. If I ever have a conflict of interest in future posts I will remind readers."
  },
  {
    "objectID": "posts/building_a_workstation_pc/index.html#pc-parts-list",
    "href": "posts/building_a_workstation_pc/index.html#pc-parts-list",
    "title": "Building a PC for informatics!",
    "section": "PC parts list",
    "text": "PC parts list\nWithout discounts, you can expect the following to cost ~$2000. Parts list: https://pcpartpicker.com/list/n7KQGJ\n\nMotherboard: MSI MAG B650 TOMAHAWK WIFI\nCPU: AMD Ryzen 9 7950X 16-Core Processor\nRAM: Corsair Vengeance 96 GB DDR5 (48 GB X 2)\nCPU Cooler: Arctic Liquid Freezer III 360\nCase: Lian Li Lancool II MESH Type C\nPSU: Corsair RM850e 850W\nGPU: ASUS Dual GeForce RTX 3060\nMain Drive: Samsung 990 PRO PCIe 4.0 NVMe M.2 SSD (4 TB)\nExtra Drives: WD RED Plus 8TB 3.5 in HDD"
  },
  {
    "objectID": "posts/building_a_workstation_pc/index.html#assembly",
    "href": "posts/building_a_workstation_pc/index.html#assembly",
    "title": "Building a PC for informatics!",
    "section": "Assembly",
    "text": "Assembly\nIf you‚Äôve never built a PC before it‚Äôs honestly not that hard once you have compatible parts all picked out. Just follow the manuals and look things up if you are confused, it‚Äôs just like working in the lab. Remember the first time you ran a PCR?\nBecause it‚Äôs 2025 and there‚Äôs a YouTube video for everything, I‚Äôd suggest this one which covers all the steps."
  },
  {
    "objectID": "posts/building_a_workstation_pc/index.html#install-ubuntu",
    "href": "posts/building_a_workstation_pc/index.html#install-ubuntu",
    "title": "Building a PC for informatics!",
    "section": "Install Ubuntu",
    "text": "Install Ubuntu\nFor the past 5 years I‚Äôve managed to run my day to to day and 90% of my bioinformatics tasks (minus a couple mass spec/NMR tools) from an Ubuntu Linux based system. If you are doing primarily mass spectrometry or NMR or use mostly Windows-only software you can either stop here and just install Windows, or install Linux and Windows side-by-side in dual boot. But here we are sticking with Ubuntu only.\nTo create a bootable USB stick and install Ubuntu on the compute follow the directions here.\nNote: Be very careful during installation if you are installing Ubuntu onto a hard drive that already has data on it. Ideally make a backup of that data first and then follow all of the steps carefully as there is the potentiall to erase everything on it."
  },
  {
    "objectID": "posts/building_a_workstation_pc/index.html#update-linux-packages",
    "href": "posts/building_a_workstation_pc/index.html#update-linux-packages",
    "title": "Building a PC for informatics!",
    "section": "Update Linux packages",
    "text": "Update Linux packages\nOpen the terminal and update the default packages.\nsudo apt get update\nsudo apt update"
  },
  {
    "objectID": "posts/building_a_workstation_pc/index.html#install-software-i-use-often-globally",
    "href": "posts/building_a_workstation_pc/index.html#install-software-i-use-often-globally",
    "title": "Building a PC for informatics!",
    "section": "Install software I use often, globally",
    "text": "Install software I use often, globally\n\ncurl is used by almost everything that interacts with the internet. If you‚Äôve ever seen this xkcd cartoon, it‚Äôs referencing curl.\ngit is specific to software development, you may or may not want it.\nncdu is useful for finding where large files are on your hard drive(s).\n\nSeperate commands below so it‚Äôs easier to modify\nsudo apt install curl\nsudo apt install ncdu\nsudo apt install git\n\ncoolercontrol is something I am trying out for the first time.\n\n# optional, allows you to monitor the temps of your PC components\nsudo apt install coolercontrol\nsudo systemctl enable coolercontrold --now\nTo setup git to work with your GitHub account follow the directions to install gh for easier authentication: https://github.com/cli/cli/blob/trunk/docs/install_linux.md\nThen run gh auth login and follow the prompts."
  },
  {
    "objectID": "posts/building_a_workstation_pc/index.html#install-micromamba",
    "href": "posts/building_a_workstation_pc/index.html#install-micromamba",
    "title": "Building a PC for informatics!",
    "section": "Install micromamba",
    "text": "Install micromamba\nMamba is faster than Conda and micromamba is mamba but‚Ä¶ micro. Lately, I‚Äôve been playing around with pixi (and all this blog‚Äôs posts use it), but I haven‚Äôt yet downloaded it to this new computer.\n\n\"${SHELL}\" &lt;(curl -L micro.mamba.pm/install.sh)\nmicromamba"
  },
  {
    "objectID": "posts/building_a_workstation_pc/index.html#create-a-python-3.12-conda-environment",
    "href": "posts/building_a_workstation_pc/index.html#create-a-python-3.12-conda-environment",
    "title": "Building a PC for informatics!",
    "section": "Create a Python 3.12 Conda environment",
    "text": "Create a Python 3.12 Conda environment\nmicromamba activate\nmicromamba create --name py312 python=3.12\nmicromamba activate py312\npip install bpytop rich"
  },
  {
    "objectID": "posts/building_a_workstation_pc/index.html#modify-.bashrc",
    "href": "posts/building_a_workstation_pc/index.html#modify-.bashrc",
    "title": "Building a PC for informatics!",
    "section": "Modify .bashrc",
    "text": "Modify .bashrc\nI launch the Python REPL in the the terminal very frequently. The following just saves me time by aliasing ‚Äòp‚Äô in Bash to mean ‚Äòpython‚Äô. So, I open a terminal, type p, hit enter and I‚Äôm inside Python‚Äôs REPL.\necho \"\nalias p='python'\n\" &gt;&gt; ~/.bashrc"
  },
  {
    "objectID": "posts/building_a_workstation_pc/index.html#run-something-every-time-python-is-opened",
    "href": "posts/building_a_workstation_pc/index.html#run-something-every-time-python-is-opened",
    "title": "Building a PC for informatics!",
    "section": "Run something every time Python is opened",
    "text": "Run something every time Python is opened\nI‚Äôm not a fan of startup imports/scripts in langauages like Python and R. However, for Python I find myself using Rich‚Äôs inspect so often that I have made it auto import.\necho \"\"\"try:\n    from rich import inspect\n    print(\\\"Imported 'from rich import inspect' as instructed by \\$HOME/.pythonrc\\\")\nexcept ImportError:\n    pass\n\"\"\" &gt; $HOME/.pythonrc\necho \"\nexport PYTHONSTARTUP=$HOME/.pythonrc\n\" &gt;&gt; ~/.bashrc"
  },
  {
    "objectID": "posts/building_a_workstation_pc/index.html#install-docker",
    "href": "posts/building_a_workstation_pc/index.html#install-docker",
    "title": "Building a PC for informatics!",
    "section": "Install Docker",
    "text": "Install Docker\nInstructions for installing Docker were taken from here.\n# Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n# Install docker and components\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n# Test\nsudo docker run hello-world"
  },
  {
    "objectID": "posts/building_a_workstation_pc/index.html#make-docker-run-without-sudo",
    "href": "posts/building_a_workstation_pc/index.html#make-docker-run-without-sudo",
    "title": "Building a PC for informatics!",
    "section": "Make Docker run without sudo",
    "text": "Make Docker run without sudo\nCommands from https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user\nsudo groupadd docker\nsudo usermod -aG docker $USER\n# get out of sudo mode with `sudo -k`\nsudo -k\nnewgrp docker\ndocker run hello-world\nIf docker run hello-world results in permission issues you may need to restart the computer then try again."
  },
  {
    "objectID": "posts/building_a_workstation_pc/index.html#other",
    "href": "posts/building_a_workstation_pc/index.html#other",
    "title": "Building a PC for informatics!",
    "section": "Other",
    "text": "Other\nI just used Ubuntu‚Äôs App Center application to install VScode and GitKraken. Maybe I‚Äôll regret this later (in the past I‚Äôve run into snags using Snap)."
  },
  {
    "objectID": "posts/intro_to_mass_spec/2_mzml/index.html",
    "href": "posts/intro_to_mass_spec/2_mzml/index.html",
    "title": "Part 2: Anatomy of an mzXML/mzML file",
    "section": "",
    "text": "This is a continuation from Part 1.\n\nIntroduction\nAs hinted by the name of its predecessor (mzXML), mzML is an XML file, which is a highly-structured ‚ÄúMarkup Language‚Äù. The current specifications for mzML, as well as example files, can be found over at https://www.psidev.info/mzml.\nTake a second and go look at an example mzML file over on HUPO-PSI‚Äôs GitHub repo. The main thing to notice is that it is highly structured and there‚Äôs a lot of additional metadata contained in this file beyond m/z and intensity. This can be important for certain analysis (e.g.¬†for MALDI, the metadata will contain sample location information).\n\n\nHow to approach reading the file\nIf you happen to have experience with HTML code it‚Äôs quite similar to XML. The important thing to note is there are ‚Äútags‚Äù that the denote the start and end of certain info and these can be nested within each other. For example:\n1&lt;dataProcessing id=\"Xcalibur Processing\" softwareRef=\"Xcalibur\"&gt;\n2    &lt;processingMethod order=\"1\"&gt;\n3        &lt;cvParam cvLabel=\"MS\" accession=\"MS:1000033\" name=\"deisotoping\" value=\"false\"/&gt;\n4        &lt;cvParam cvLabel=\"MS\" accession=\"MS:1000034\" name=\"charge deconvolution\" value=\"false\"/&gt;\n5        &lt;cvParam cvLabel=\"MS\" accession=\"MS:1000035\" name=\"peak picking\" value=\"true\"/&gt;\n6    &lt;/processingMethod&gt;\n7&lt;/dataProcessing&gt;\n\n1\n\nOpens the dataProcessing tag and defines the properties of ‚Äòid‚Äô and ‚ÄòsoftwareRef‚Äô; the last line &lt;/dataProcessing&gt; closes the ‚ÄúdataProcessing‚Äù tag.\n\n2\n\nDefines the processingMethod with the property order=\"1\".\n\n3\n\nDefines a cvParam tag which uses properties from the controlled mzML ontology to let us know no deisotoping was performed.\n\n4\n\nDefines a cvParam tag which uses properties from the controlled mzML ontology to let us know no charge deconvolution was performed.\n\n5\n\nDefines a cvParam tag which uses properties from the controlled mzML ontology to let us know peak peaking was performed.\n\n6\n\nCloses the processingMethod tag.\n\n7\n\nCloses the dataProcessing tag.\n\n\nUsually mzML files are indented which allows you to easily discern which tags are nested under which other tags; but there is technically no requirement that there be indentations.\n\n\nWhere are the spectra?\nAnother thing you may have noticed is that there are no obvious spectra in this mzML file. No table, comma separated numbers, nothin‚Äô.\nThe spectra are indeed there, just kind of hidden between the &lt;spectrumList count=\"2\"&gt;... &lt;spectrumList&gt; tags (L108-201). The &lt;spectrumList count=\"2\"&gt; tag tells us that we can expect two spectra and below I‚Äôll walk you through how to read one of those.\nHere are lines 109-45 of the mzML file denoting a single MS spectrum.\n1&lt;spectrum index=\"0\" id=\"S19\" nativeID=\"19\" defaultArrayLength=\"10\"&gt;\n2  &lt;cvParam cvRef=\"MS\" accession=\"MS:1000580\" name=\"MSn spectrum\" value=\"\"/&gt;\n3  &lt;cvParam cvRef=\"MS\" accession=\"MS:1000511\" name=\"ms level\" value=\"1\"/&gt;\n4  &lt;spectrumDescription&gt;\n    &lt;cvParam cvRef=\"MS\" accession=\"MS:1000127\" name=\"centroid mass spectrum\" value=\"\"/&gt;\n    &lt;cvParam cvRef=\"MS\" accession=\"MS:1000528\" name=\"lowest m/z value\" value=\"400.39\"/&gt;\n    &lt;cvParam cvRef=\"MS\" accession=\"MS:1000527\" name=\"highest m/z value\" value=\"1795.56\"/&gt;\n    &lt;cvParam cvRef=\"MS\" accession=\"MS:1000504\" name=\"base peak m/z\" value=\"445.347\"/&gt;\n    &lt;cvParam cvRef=\"MS\" accession=\"MS:1000505\" name=\"base peak intensity\" value=\"120053\"/&gt;\n    &lt;cvParam cvRef=\"MS\" accession=\"MS:1000285\" name=\"total ion current\" value=\"16675500\"/&gt;\n    &lt;scan instrumentConfigurationRef=\"LCQDeca\"&gt;\n      &lt;referenceableParamGroupRef ref=\"CommonMS1SpectrumParams\"/&gt;\n      &lt;cvParam cvRef=\"MS\" accession=\"MS:1000016\" name=\"scan time\" value=\"5.8905\" unitAccession=\"MS:1000038\" unitName=\"minute\"/&gt;\n      &lt;cvParam cvRef=\"MS\" accession=\"MS:1000512\" name=\"filter string\" value=\"+ c NSI Full ms [ 400.00-1800.00]\"/&gt;\n      &lt;scanWindowList count=\"1\"&gt;\n        &lt;scanWindow&gt;\n          &lt;cvParam cvRef=\"MS\" accession=\"MS:1000501\" name=\"scan m/z lower limit\" value=\"400\"/&gt;\n          &lt;cvParam cvRef=\"MS\" accession=\"MS:1000500\" name=\"scan m/z upper limit\" value=\"1800\"/&gt;\n        &lt;/scanWindow&gt;\n      &lt;/scanWindowList&gt;\n    &lt;/scan&gt;\n  &lt;/spectrumDescription&gt;\n5  &lt;binaryDataArrayList count=\"2\"&gt;\n6    &lt;binaryDataArray arrayLength=\"10\" encodedLength=\"108\" dataProcessingRef=\"XcaliburProcessing\"&gt;\n7      &lt;cvParam cvRef=\"MS\" accession=\"MS:1000523\" name=\"64-bit float\" value=\"\"/&gt;\n8      &lt;cvParam cvRef=\"MS\" accession=\"MS:1000576\" name=\"no compression\" value=\"\"/&gt;\n9      &lt;cvParam cvRef=\"MS\" accession=\"MS:1000514\" name=\"m/z array\" value=\"\"/&gt;\n10      &lt;binary&gt;AAAAAAAAAAAAAAAAAADwPwAAAAAAAABAAAAAAAAACEAAAAAAAAAQQAAAAAAAABRAAAAAAAAAGEAAAAAAAAAcQAAAAAAAACBAAAAAAAAAIkA=&lt;/binary&gt;\n11    &lt;/binaryDataArray&gt;\n12    &lt;binaryDataArray arrayLength=\"10\" encodedLength=\"108\" dataProcessingRef=\"XcaliburProcessing\"&gt;\n      &lt;cvParam cvRef=\"MS\" accession=\"MS:1000523\" name=\"64-bit float\" value=\"\"/&gt;\n      &lt;cvParam cvRef=\"MS\" accession=\"MS:1000576\" name=\"no compression\" value=\"\"/&gt;\n      &lt;cvParam cvRef=\"MS\" accession=\"MS:1000515\" name=\"intensity array\" value=\"\"/&gt;\n      &lt;binary&gt;AAAAAAAAJEAAAAAAAAAiQAAAAAAAACBAAAAAAAAAHEAAAAAAAAAYQAAAAAAAABRAAAAAAAAAEEAAAAAAAAAIQAAAAAAAAABAAAAAAAAA8D8=&lt;/binary&gt;\n    &lt;/binaryDataArray&gt;\n13  &lt;/binaryDataArrayList&gt;\n14&lt;/spectrum&gt;\n\n1\n\nOpens the spectrum tag and tells us the spectrum within should have 10 m/z data points.\n\n2\n\nIt is part of an MSn experiment.\n\n3\n\nAnd it represents an MS1 acquisition.\n\n4\n\nThis block (click bullet number to left of this text to highlight) contains summary metadata about the spectrum and its acquisition.\n\n5\n\nThis line informs us that there are two binary arrays within this tag/section.\n\n6\n\nOpen tag of a single data array (i.e.¬†list). Has properties telling us array is a binary string 108 characters long, there will be 10 data points when decoded, and was created with Thermo‚Äôs XcaliburProcessing software\n\n7\n\nThe data points will be 64-bit floating point numbers (i.e.¬†numbers precise to ~16 decimals).\n\n8\n\nThe data is not compressed.\n\n9\n\nThis tag/section informs us that the binary array contains the m/z values for this spectrum.\n\n10\n\nThe binary data which, when decoded, will contain 10 m/z data points. (ie half the data of the spectrum)\n\n11\n\nEnd of the first data array.\n\n12\n\nThe second data array can be ready the same as the first, but contains the intensity value data points.\n\n13\n\nCloses the binaryDataArrayList.\n\n14\n\nCloses the first spectrum.\n\n\nI‚Äôll leave it to the reader to look through the second spectrum (lines 146-200) which contains MS2 data from the fragmentation of a 445.34 ion.\n\n\nNext\nIn the next post we will dive into some actual code and how to work with LC-MS/MS data using the R programming language.\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "posts/intro_to_mass_spec/1_intro/index.html",
    "href": "posts/intro_to_mass_spec/1_intro/index.html",
    "title": "Part 1: Mass Spectrometry Data",
    "section": "",
    "text": "Mass spectrometry has become an integral analytical technique in natural product discovery, both in measuring accurate mass for chemical formula determination, to analyzing molecule fragmentation for structure elucidation and library searches.\nI won‚Äôt go in to too much detail here but there are some important experimental considerations when approaching the analysis of mass spectrometry data.\nFor more information I‚Äôd recommended looking at learning material the big vendors usually have, such as this.\n\n\n\n\nThere are a number of ionizers that are in use in modern mass spectrometers, with electrospray ionization (ESI) being the most common in our field. Nethod of ionization is important to consider because it will effect which types molecules are ionized, how they are ionized, whether the molecules will remain largely intact or fragmented, and what types of adducts you can expect. For ESI instruments are often in run in positive (most often) or negative mode, and sometimes both (polarity switching) and you should be aware of this going into your analysis (or be prepared to extract the relevant metadata from the raw data).\n\n\n\n‚ÄúMass spectrometry‚Äù is ultimately performed by the mass analyzer, the part that separates, filters, differentiates molecules based on mass to charge (and sometimes size/shape via ion-mobility). There are a number of analyzers on the market, with the most popular being quadrupoles, ion traps, orbitraps, time-of-flight, and combinations thereof. The type of analyzer is important to consider in the analysis as well, and the following should be thought about when approaching a new analysis.\n\nWhat is the resolving power of the anlyzer(s)? Plural because, for example, the m/z filter window of a quadrupole that filters into a Time-Of-Flight (TOF) may or may not impact your analysis.\nWhat mode was the instrument run in (e.g.¬†for triple quads was it run in precursor ion scan, neutral loss scan, product ion scan and MRM/SRM mode?)\nOften analyzers will have their efficacy rated in FWHM (full width at half maximum) which is a measure of resolving power\nIf you confuse resolving power with mass resolution you aren‚Äôt alone, there‚Äôs been much controversy over the years as they somewhat related. See this whitepaper by Agilent. Simply stated, resolving power measures how well you can separate two mass peaks in a spectrum and resolution is a measure of how ‚Äúwide‚Äù your peaks are.\nWhat is the scan speed of your analyzer(s)?\nSome analyzers are very fast (i.e.¬†they can analyze/separate many m/z per second), while others are slower. Some sacrifice sensitivity, accuracy, resolution, etc for higher scan speed. All things to be aware of.\n\n\n\n\nWhile there are different detectors, this isn‚Äôt usually a concern during modern analyses. However, if you notice things like sensitivity being too high or low it could be good feedback to give to the instrument operator as it could be detector settings (though llikely sample concentration or ionization efficiency).\nAnother thing to note is that some instruments may have more than one detector and they may serve different purposes. For example, some Time-Of-Flight (TOF) instruments have both a ‚Äúlinear‚Äù detector and a ‚Äúreflectron‚Äù detector that elongates the flight path allowing higher resolving power but lower m/z ceiling than the ‚Äúlinear‚Äù detector."
  },
  {
    "objectID": "posts/intro_to_mass_spec/1_intro/index.html#the-instrument",
    "href": "posts/intro_to_mass_spec/1_intro/index.html#the-instrument",
    "title": "Part 1: Mass Spectrometry Data",
    "section": "",
    "text": "There are a number of ionizers that are in use in modern mass spectrometers, with electrospray ionization (ESI) being the most common in our field. Nethod of ionization is important to consider because it will effect which types molecules are ionized, how they are ionized, whether the molecules will remain largely intact or fragmented, and what types of adducts you can expect. For ESI instruments are often in run in positive (most often) or negative mode, and sometimes both (polarity switching) and you should be aware of this going into your analysis (or be prepared to extract the relevant metadata from the raw data).\n\n\n\n‚ÄúMass spectrometry‚Äù is ultimately performed by the mass analyzer, the part that separates, filters, differentiates molecules based on mass to charge (and sometimes size/shape via ion-mobility). There are a number of analyzers on the market, with the most popular being quadrupoles, ion traps, orbitraps, time-of-flight, and combinations thereof. The type of analyzer is important to consider in the analysis as well, and the following should be thought about when approaching a new analysis.\n\nWhat is the resolving power of the anlyzer(s)? Plural because, for example, the m/z filter window of a quadrupole that filters into a Time-Of-Flight (TOF) may or may not impact your analysis.\nWhat mode was the instrument run in (e.g.¬†for triple quads was it run in precursor ion scan, neutral loss scan, product ion scan and MRM/SRM mode?)\nOften analyzers will have their efficacy rated in FWHM (full width at half maximum) which is a measure of resolving power\nIf you confuse resolving power with mass resolution you aren‚Äôt alone, there‚Äôs been much controversy over the years as they somewhat related. See this whitepaper by Agilent. Simply stated, resolving power measures how well you can separate two mass peaks in a spectrum and resolution is a measure of how ‚Äúwide‚Äù your peaks are.\nWhat is the scan speed of your analyzer(s)?\nSome analyzers are very fast (i.e.¬†they can analyze/separate many m/z per second), while others are slower. Some sacrifice sensitivity, accuracy, resolution, etc for higher scan speed. All things to be aware of.\n\n\n\n\nWhile there are different detectors, this isn‚Äôt usually a concern during modern analyses. However, if you notice things like sensitivity being too high or low it could be good feedback to give to the instrument operator as it could be detector settings (though llikely sample concentration or ionization efficiency).\nAnother thing to note is that some instruments may have more than one detector and they may serve different purposes. For example, some Time-Of-Flight (TOF) instruments have both a ‚Äúlinear‚Äù detector and a ‚Äúreflectron‚Äù detector that elongates the flight path allowing higher resolving power but lower m/z ceiling than the ‚Äúlinear‚Äù detector."
  },
  {
    "objectID": "posts/intro_to_mass_spec/1_intro/index.html#raw-data-formats",
    "href": "posts/intro_to_mass_spec/1_intro/index.html#raw-data-formats",
    "title": "Part 1: Mass Spectrometry Data",
    "section": "Raw data formats",
    "text": "Raw data formats\nUnfortunately different instrument vendors, and even different instruments from the same vendor, have their own unique data storage format. This is for a variety of good and bad reasons, the most convincing to me being that instruments with ever-increasing acquisition speeds and ever-increasing data size need faster/better software/hardware strategies to store data, which can provide a competitive advantage.\nRaw data is llikely to come with file extensions (.wiff, .d, .raw/.RAW, .lcd, etc.) and some are locked in so that only the instrument vendor‚Äôs software can read the data."
  },
  {
    "objectID": "posts/intro_to_mass_spec/1_intro/index.html#open-source-data-formats",
    "href": "posts/intro_to_mass_spec/1_intro/index.html#open-source-data-formats",
    "title": "Part 1: Mass Spectrometry Data",
    "section": "Open-source data formats",
    "text": "Open-source data formats\nFortunately there are widely used, open, standard formats available. You will proabably encounter mzXML and/or its newer version, mzML; so, go with mzML if you have the option. mzXML files have the file extension mzXML and mzML files have the file extension .mzXML and .mzML.\nSome vendor software allows converting a file in proprietary data format to mzML, otherwise your best bet is llikely the program msconvert available as part of the ProteoWizard software library. Unfortunately some vendor formats can only be converted on a Windows computer, a limitation of vendors only providing Windows-based DLLs (i.e.¬†don‚Äôt complain to the ProteoWizard team about this).\nmsconvert can be used from both its GUI or at the command line\nFor an example of how to use the command line you can take a look at this zip of a directory that contains a batch file that converts a large number of files at once https://ccms-ucsd.github.io/GNPSDocumentation/fileconversion/#data-conversion-easy\nI haven‚Äôt had the chance to try them but supposedly there are some relatively new Docker containers that can successfully run msconvert. If you know how badly this was needed then you know how exciting this would be/is.\nGoing forware I will only cover mzML/mzXML as they are by far the most commonly encountered open formats in our field. Other formats can be seen at https://www.psidev.info/specifications; and MGF at http://www.matrixscience.com/help/data_file_help.html"
  },
  {
    "objectID": "posts/chemsim_2024_02_28/index.html",
    "href": "posts/chemsim_2024_02_28/index.html",
    "title": "Chemical similarity with Python and RDKit",
    "section": "",
    "text": "I recently reworked a portion of the SocialGene codebase to allow for easier addons/plugins. Inevitably, this also meant I needed to write additional code to accommodate chemistry into software that had primarily been genomics oriented. I won‚Äôt go into details here but chemicals from any source (e.g.¬†NPAtlas, GNPS standards, etc.) are linked to a standardized notion of a molecule (i.e.¬†the same molecule from two sources will link to a single ‚Äúchemical‚Äù node). But I also wanted chemical similarity to be incorporated. After incorporating some basic chemical similarity measures I realized that some of the molecules I wanted to see linked as ‚Äúsimilar‚Äù weren‚Äôt and that led me down a rabbit hole of similarity measures. Long story short, there‚Äôs might need to be at least two, Tanimoto and a semi-custom MCS similarity measure (at the bottom of this post). However, the second takes a long time to compute, so I decided to pump out this quick, very introductory post on calculating chemical similarity."
  },
  {
    "objectID": "posts/chemsim_2024_02_28/index.html#calculate-similarity",
    "href": "posts/chemsim_2024_02_28/index.html#calculate-similarity",
    "title": "Chemical similarity with Python and RDKit",
    "section": "Calculate similarity",
    "text": "Calculate similarity\nJust like with fingerprint/vector generation, there are a lot of different methods to measure similarity between chemical vectors. For simplicity I‚Äôm just going to use RDKit‚Äôs Tanimoto score.\nAnd, obviously, if we calculate the similarity between the same vector we should get the best score possible which, for Tanimoto, is 1. (Tanimoto scores range between 0 and 1)\n\nDataStructs.TanimotoSimilarity(mol1_morgan_fingerprint, mol1_morgan_fingerprint)\n\n1.0"
  },
  {
    "objectID": "posts/chemsim_2024_02_28/index.html#caveat",
    "href": "posts/chemsim_2024_02_28/index.html#caveat",
    "title": "Chemical similarity with Python and RDKit",
    "section": "Caveat",
    "text": "Caveat\nAs of writing this post, NPAtlas contains ~30,000 molecules. Using RDKit‚Äôs ‚Äúbulk‚Äù versions of calculating Tanimoto similarity, an all-vs-all similarity takes a few minutes, running in parallel on a 12-core processor (24 logical cores). However, MCS is a lot slower of a calculation. To calculate this MCS similarity on NPatlas required substantial time on CHTC, UW-Madison‚Äôs Center for High Throughput Computing. Enough time for me to write this post, and more. MCS has a number of settings and it‚Äôs not clear yet if the ones I have chosen are adequate or best for what I was hoping to achieve."
  }
]